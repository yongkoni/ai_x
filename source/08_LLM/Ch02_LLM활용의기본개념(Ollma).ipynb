{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eecccd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.container{width:86% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{fontsize:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}))\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<style>\n",
    "div.container{width:86% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{fontsize:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}))\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b973fe",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch02 LLM 활용의 기본 개념 (ollama)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea5e81",
   "metadata": {},
   "source": [
    "# 1. LLM을 활용하여 답변 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220a115",
   "metadata": {},
   "source": [
    "## 1) Ollama 이용한 로컬 LLM 이용\n",
    "\n",
    "- 성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536c560",
   "metadata": {},
   "source": [
    "### ⓐ ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- ollama pull deepseek-r1:1.5b (window키 + R → powershell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e46cff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nKorea, officially known as the Democratic Republic of the People's Republic of China, has its own capital city, which is Busan.\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-06-26T02:05:32.2793003Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4327560900, 'load_duration': 2475714100, 'prompt_eval_count': 10, 'prompt_eval_duration': 227814800, 'eval_count': 33, 'eval_duration': 1619905600, 'model_name': 'deepseek-r1:1.5b'}, id='run--63d591bf-72b2-40ff-b2c4-44a8dca96d15-0', usage_metadata={'input_tokens': 10, 'output_tokens': 33, 'total_tokens': 43})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"deepseek-r1:1.5b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result  # 추론모델 <think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2d2f4",
   "metadata": {},
   "source": [
    "### ⓑ ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- ollama pull llama3.2:1b(window키 + R → powershell)\n",
    "- llama : 공식적으로 한글지원 안 됨 (llama3.1 405b 한글지원 가능 → llama3.3 70b)\n",
    "- exaone : 공식적으로 한글지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc747d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of South Korea is Seoul. However, it's worth noting that North Korea also claims Seoul as its own capital, and the two countries have a complex relationship due to their separate governments and histories. But in modern times, Seoul is generally considered the de facto capital of South Korea.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:05:40.3884841Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6839475500, 'load_duration': 2942416600, 'prompt_eval_count': 32, 'prompt_eval_duration': 447239100, 'eval_count': 59, 'eval_duration': 3449244800, 'model_name': 'llama3.2:1b'}, id='run--17a90002-3c1e-4b63-942f-6b3911e493f6-0', usage_metadata={'input_tokens': 32, 'output_tokens': 59, 'total_tokens': 91})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54ff20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of South Korea is Seoul. However, it's worth noting that North Korea also claims Seoul as its own capital, and the two countries have a complex relationship due to their separate governments and histories. But in modern times, Seoul is generally considered the de facto capital of South Korea.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6422dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"수도는 대략 한국의 주부에서 동남아시아 지역에 있는 các 도시로, 이들 중 일부는 국가의 공식 수도 인 首都과 반대 방향으로 정의됩니다.\\n\\n이러한 casos에는 다음과 같습니다:\\n\\n- Seoul (한국의 수도) : 한국의 국가의 공식 수도입니다.\\n- Busan (한국의 2 위)\\n- Daejeon (한국의 3 위)\\n\\n이러한 경우, 지역은 '수도'를 제외하고 '공화국 수도' 또는 '공화국 주부'로 정의됩니다.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:05:48.0586701Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7635206200, 'load_duration': 29572300, 'prompt_eval_count': 32, 'prompt_eval_duration': 255846000, 'eval_count': 124, 'eval_duration': 7349270900, 'model_name': 'llama3.2:1b'}, id='run--60d01ad7-bfd6-4b91-a4c5-6009f6032716-0', usage_metadata={'input_tokens': 32, 'output_tokens': 124, 'total_tokens': 156})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"한국 수도는 어디예요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafe3a2",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29853f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model = \"gpt-4o-mini\")\n",
    "# result = llm.invoke(\"What is the capital of Korea?\")\n",
    "# result → 에러 이유 : OPENAI_API_KEY 환경변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe17dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 가져오기\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "load_dotenv()\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기(.env못씀)\n",
    "# 보안키 추가 후 \n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2668095c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='한국의 수도는 서울입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 18, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BmWHl7YIKwZ1JaNfZl1xKylLy6ZRj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0821b77d-cc6a-4dcc-a946-b128c9db347b-0', usage_metadata={'input_tokens': 18, 'output_tokens': 7, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4.1-nano\", \n",
    "                # openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "                )\n",
    "llm.invoke(\"What is the capital of Korea? Answer me in Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e645af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPENAI_API_KEY는 아님\n",
    "# Claude → Anthropic\n",
    "# Azure, upstage, Bedrock : 에러 메세지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model = \"gpt-4o-mini\")\n",
    "# 에러를 내면 OPENAI_API_VERSION 환경변수가 필요하는 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc55ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model = \"claude-3-5-sonnet-20240620\")\n",
    "# llm.invoke(\"What is the capital of Korea?\")\n",
    "# 에러 메세지를 봐도 환경변수 이름을 알 수 X → ChatAnthropic 검색후, \n",
    "# langchain docs에서 명시한 ANTHROPIC_API_KEY 이름의 환경변수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4a7f1",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "\n",
    "- 프롬프트 : llm호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2809759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# 프로프트 타입 : 스트링, PromptValue, BaseMessage리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b83671",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffadba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Korea is Seoul.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:05:53.0982589Z', 'done': True, 'done_reason': 'stop', 'total_duration': 744058300, 'load_duration': 20354200, 'prompt_eval_count': 31, 'prompt_eval_duration': 252663100, 'eval_count': 9, 'eval_duration': 470776100, 'model_name': 'llama3.2:1b'}, id='run--1ee71c9e-a234-4801-a758-9bbe0fb051c0-0', usage_metadata={'input_tokens': 31, 'output_tokens': 9, 'total_tokens': 40})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\",  # {}안의 값을 새로운 값으로 대입 가능\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c6fe9",
   "metadata": {},
   "source": [
    "## 2) 메세지 기반 프롬프트 작성\n",
    "\n",
    "- BaseMessage 리스트\n",
    "- BaseMessage 상속 받은 클래스 : AIMessage, HummanMessage, SystemMessage, ToolMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9fb95d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I think there may be a mistake here. The capital of France is actually Paris. It's a well-known and historic city that serves as the country's seat of government.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:05:57.1607505Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3529892000, 'load_duration': 18891400, 'prompt_eval_count': 86, 'prompt_eval_duration': 1367779800, 'eval_count': 36, 'eval_duration': 2141667500, 'model_name': 'llama3.2:1b'}, id='run--4307e74f-ee9f-412c-b912-e49a5afaaf9c-0', usage_metadata={'input_tokens': 86, 'output_tokens': 36, 'total_tokens': 122})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content = \"You ara a helpful assistant!\"),\n",
    "    HumanMessage(content = \"What is the capital of Italy?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content = \"What is the capital of Korea?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content = \"What is the capital of France?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc649362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Seoul.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# BaseMessage list로 하면 렝체인화 X, ChatPromptTemplate X\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content = \"You ara a helpful assistant!\"),\n",
    "    HumanMessage(content = \"What is the capital of Italy?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content = \"What is the capital of Korea?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content = \"What is the capital of {country}?\")\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(message_list)\n",
    "prompt = chatPromptTemplate.invoke({'country':'Korea'})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182eef4",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "\n",
    "- BaseMessage 리스트 → 튜플 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fb175a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?중국\n",
      "프롬프트 : messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of 중국?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of China is Beijing.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You ara a helpful assistant!\"),\n",
    "    (\"human\", \"What is the capital of {country}?\"),\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":country})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecabbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?프랑스\n",
      "프롬프트 : messages=[SystemMessage(content='당신은 대한민국 전문 도우미야!', additional_kwargs={}, response_metadata={}), HumanMessage(content='프랑스의 수도가 어디예요?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프랑스의 수도는 파리입니다.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:06:07.5832953Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1114201800, 'load_duration': 25382300, 'prompt_eval_count': 44, 'prompt_eval_duration': 565730300, 'eval_count': 10, 'eval_duration': 522096800, 'model_name': 'llama3.2:1b'}, id='run--f732b413-371c-473b-90c3-92b780db2833-0', usage_metadata={'input_tokens': 44, 'output_tokens': 10, 'total_tokens': 54})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 대한민국 전문 도우미야!\"),\n",
    "    (\"human\", \"{country}의 수도가 어디예요?\"),\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":country})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab7bd7",
   "metadata": {},
   "source": [
    "# 3. 답변 형식을 컨트롤하기\n",
    "\n",
    "- invoke 실행결과는 AIMessage() → String이나 json, 객체 : outputParser이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc926012",
   "metadata": {},
   "source": [
    "## 1) 문자열 출력 파서 이용\n",
    "\n",
    "- StrOutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66999f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 :  text='What is the capital of Korea. Return the name of the city only'\n",
      "llm 결과 : <class 'langchain_core.messages.ai.AIMessage'> content='Seoul' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:06:08.2112782Z', 'done': True, 'done_reason': 'stop', 'total_duration': 594739100, 'load_duration': 23952800, 'prompt_eval_count': 39, 'prompt_eval_duration': 454072000, 'eval_count': 3, 'eval_duration': 116133600, 'model_name': 'llama3.2:1b'} id='run--545082e6-90ce-4ce2-9576-e147cd2a19ef-0' usage_metadata={'input_tokens': 39, 'output_tokens': 3, 'total_tokens': 42}\n",
      "파서 결과 : Seoul\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print('프롬프트 : ',prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print('llm 결과 :', type(result), result)\n",
    "\n",
    "# 문자열 출력 파서를 이용하여 llm응답을 단순 문자열 변환\n",
    "output_parser = StrOutputParser()\n",
    "print('파서 결과 :',output_parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8e16922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca3ff376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate(변수설정) → ChatPromptTemplate(변수설정, system과 모법답안 지정)\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant with expertise in South Korea.\"),\n",
    "    (\"human\", \"What is the capital of {country}? Return the name if the city only.\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1820ae9",
   "metadata": {},
   "source": [
    "## 2) 응답 타입 확인\n",
    "\n",
    "- json()으로 응답하기를 원하지만, 우선 어떤 형식으로 반환되는지 확인\n",
    "- {\"name\":\"흥\", \"age\":22}(json) / {\"name\":\"흥\", \"age\":22}(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cd73163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'> text='Give following information about Korea\\n    - Capital\\n    - Population\\n    - Language\\n    - Currency\\n    retrun it is JSON format and return the JSON dictionary only\\n    '\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'> content='```json\\n{\\n  \"capital\": \"Seoul\",\\n  \"population\": 51000000,\\n  \"language\": \"Korean\",\\n  \"currency\": \"South Korean won\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T02:06:13.2480223Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3436599500, 'load_duration': 23577700, 'prompt_eval_count': 62, 'prompt_eval_duration': 949803700, 'eval_count': 41, 'eval_duration': 2462587100, 'model_name': 'llama3.2:1b'} id='run--fef3fffd-19ca-421b-b131-80286d4be75c-0' usage_metadata={'input_tokens': 62, 'output_tokens': 41, 'total_tokens': 103}\n",
      "<class 'dict'> {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "count_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    retrun it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt = count_detail_prompt.invoke({\"country\":\"Korea\"})\n",
    "print(type(prompt), prompt)\n",
    "\n",
    "# Json output 파서\n",
    "output_parser = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "print(type(prompt), ai_message)\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(type(json_result), json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcaffdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Seoul',\n",
       " 'population': 51000000,\n",
       " 'language': 'Korean',\n",
       " 'currency': 'South Korean won'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    retrun it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "info = output_parser.invoke(llm.invoke(count_detail_prompt.invoke({\"country\":\"Korea\"})))\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b477186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f395d",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "\n",
    "- Pydantic 모델을 사용하여 LLM 출력을 구조화된 형식으로 받기 (JsonParser보다 훨씬 안정적)\n",
    "- Pydantic : 데이터 유효성 검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a37fa8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x0000014E2226EF20>\n"
     ]
    }
   ],
   "source": [
    "class User :\n",
    "    def __init__(self, id, name, is_active = True) :\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "user = User(\"1\", \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c18dbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel) :\n",
    "    # gt = 0 : id > 0, ge = 0 : id >= 0, lt = 0 : id < 0, le = 0 : le <= 0\n",
    "    id:int = Field(gt=0, description = \"id\")\n",
    "    name:str = Field(min_length = 2, description = \"name\")\n",
    "    is_active:bool = Field(default = True,description = \"id활성화\")\n",
    "user = User(id = \"1\", name = \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "941157e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]    \n",
    ")\n",
    "\n",
    "class CountryDetail(BaseModel) :  # description : 더 정확한 출력 유도\n",
    "    capital:str = Field(description = \"the capital of the country\")\n",
    "    population:int = Field(description = \"the population of the country\")\n",
    "    language:str = Field(description = \"the language of the country\")\n",
    "    currency:str = Field(description = \"the currency of the country\")\n",
    "\n",
    "# 출력 형식 파서 + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "# output_parser = JsonOutputParser()\n",
    "# output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"})))\n",
    "\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cddb214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital='Seoul' population=51000000 language='Korean' currency='Won'\n",
      "Seoul 51000000\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print(info.capital, info.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dded41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json : {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean\",\"currency\":\"Won\"}\n",
      "info를 dict : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'Won'}\n"
     ]
    }
   ],
   "source": [
    "print('info를 json :', info.model_dump_json())  # json()\n",
    "print('info를 dict :', info.model_dump())  # dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d394db",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 렝체인 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941d82a",
   "metadata": {},
   "source": [
    "## 1) 문자열 출력 파서 사용\n",
    "\n",
    "- invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09b7bcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\", \n",
    "                 temperature = 0)  # 일관된 답변\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc8810",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "\n",
    "- 파이프연산자(|) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63958c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트템플릿 → llm → 출력파서를 연결하는 체인 생성\n",
    "\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ddf2f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(capital_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379cbe8",
   "metadata": {},
   "source": [
    "## 3) 복합 체인 구성\n",
    "\n",
    "- 여러 단계의 추론이 필요한 경우 (체인 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0dc9b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명 → 나라명\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "country_prompt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country based on the following information:\n",
    "    {information} \n",
    "    Return the name of the the country only\"\"\",\n",
    "    input_variables=[\"information\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                                     \"This country is very famous for its wine\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f480897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라명 추측 체인 생성\n",
    "\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "# type(country_chain)\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02a14aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라설명 → ( 나라명 → ) 그 나라 수도\n",
    "\n",
    "final_chain = country_chain | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fea7a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\":country_chain} | capital_chain\n",
    "final_chain.invoke(\"This country is very famous for its wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdb0dc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "final_chain = {\"information\":RunnablePassthrough()} | \\\n",
    "                {\"country\":country_chain} | capital_chain\n",
    "final_chain.invoke(\"This country is very famous for its wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "435d66e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 템플릿에 변수 2\n",
    "# 나라 설명 → 나라명\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "country_prompt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country in the {continent} based \n",
    "    on the following information:\n",
    "    {information} \n",
    "    Return the name of the the country only\"\"\",\n",
    "    input_variables=[\"information\", \"continent\"]\n",
    ")\n",
    "# output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "#                                      \"This country is very famous for its wine\",\n",
    "#                                                       \"continent\":\"Europe\"})))\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                      \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8756b2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\" : country_chain} | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                    \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03dc82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
