{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecccd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.container{width:86% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{fontsize:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}))\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<style>\n",
    "div.container{width:86% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{fontsize:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}))\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b973fe",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch02 LLM 활용의 기본 개념 (ollama)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea5e81",
   "metadata": {},
   "source": [
    "# 1. LLM을 활용하여 답변 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220a115",
   "metadata": {},
   "source": [
    "## 1) Ollama 이용한 로컬 LLM 이용\n",
    "\n",
    "- 성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536c560",
   "metadata": {},
   "source": [
    "### ⓐ ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- ollama pull deepseek-r1:1.5b (window키 + R → powershell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e46cff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nThe capital of Korea is Paris.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-06-25T02:12:04.505073Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2387906700, 'load_duration': 1600336200, 'prompt_eval_count': 10, 'prompt_eval_duration': 238708500, 'eval_count': 12, 'eval_duration': 546033200, 'model_name': 'deepseek-r1:1.5b'}, id='run--5bc9ebd0-00fd-4072-b47e-49974a9fe9d9-0', usage_metadata={'input_tokens': 10, 'output_tokens': 12, 'total_tokens': 22})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"deepseek-r1:1.5b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result  # 추론모델 <think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2d2f4",
   "metadata": {},
   "source": [
    "### ⓑ ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- ollama pull llama3.2:1b(window키 + R → powershell)\n",
    "- llama : 공식적으로 한글지원 안 됨 (llama3.1 405b 한글지원 가능 → llama3.3 70b)\n",
    "- exaone : 공식적으로 한글지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc747d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Korea is Seoul. However, the official administrative center and the seat of government for both South and North Korea is Pyongyang in North Korea, while Seoul serves as the cultural and economic hub for the South Korean Peninsula.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T02:21:42.0902108Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4945109600, 'load_duration': 1680212700, 'prompt_eval_count': 32, 'prompt_eval_duration': 568276700, 'eval_count': 47, 'eval_duration': 2694884700, 'model_name': 'llama3.2:1b'}, id='run--844ccad1-a098-499b-b5e7-5bc5a04404f9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 47, 'total_tokens': 79})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54ff20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of South Korea is Seoul. However, the official administrative center and the seat of government for both South and North Korea is Pyongyang in North Korea, while Seoul serves as the cultural and economic hub for the South Korean Peninsula.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6422dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='아주 많습니다. 한국 수도는 주로 세인트루이스에 있습니다.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T02:22:00.1473492Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1296305900, 'load_duration': 27687100, 'prompt_eval_count': 32, 'prompt_eval_duration': 240718700, 'eval_count': 18, 'eval_duration': 1027900100, 'model_name': 'llama3.2:1b'}, id='run--5fa08b9d-e076-48c8-8770-049fe6ba52a7-0', usage_metadata={'input_tokens': 32, 'output_tokens': 18, 'total_tokens': 50})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"한국 수도는 어디예요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafe3a2",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29853f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model = \"gpt-4o-mini\")\n",
    "# result = llm.invoke(\"What is the capital of Korea?\")\n",
    "# result → 에러 이유 : OPENAI_API_KEY 환경변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe17dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 가져오기\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "load_dotenv()\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기(.env못씀)\n",
    "# 보안키 추가 후 \n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2668095c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='한국의 수도는 서울입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 18, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BmDpLhNXLHfdKJMKRzvOBqOc7fbzK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5953bb05-00e0-438d-b4e6-73c5e480e3f7-0', usage_metadata={'input_tokens': 18, 'output_tokens': 7, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4.1-nano\", \n",
    "                # openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "                )\n",
    "llm.invoke(\"What is the capital of Korea? Answer me in Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e645af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPENAI_API_KEY는 아님\n",
    "# Claude → Anthropic\n",
    "# Azure, upstage, Bedrock : 에러 메세지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d4cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model = \"gpt-4o-mini\")\n",
    "# 에러를 내면 OPENAI_API_VERSION 환경변수가 필요하는 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfc55ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model = \"claude-3-5-sonnet-20240620\")\n",
    "# llm.invoke(\"What is the capital of Korea?\")\n",
    "# 에러 메세지를 봐도 환경변수 이름을 알 수 X → ChatAnthropic 검색후, \n",
    "# langchain docs에서 명시한 ANTHROPIC_API_KEY 이름의 환경변수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4a7f1",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "\n",
    "- 프롬프트 : llm호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2809759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# 프로프트 타입 : 스트링, PromptValue, BaseMessage리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b83671",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffadba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of South Korea is Seoul. However, it's worth noting that there are two separate countries with this name: North Korea and South Korea. The capital of North Korea is Pyongyang.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T06:24:32.7948122Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6094887500, 'load_duration': 3227706600, 'prompt_eval_count': 31, 'prompt_eval_duration': 596868100, 'eval_count': 39, 'eval_duration': 2267857600, 'model_name': 'llama3.2:1b'}, id='run--a8b63d24-49d7-4c2b-b81d-4b059f911f9a-0', usage_metadata={'input_tokens': 31, 'output_tokens': 39, 'total_tokens': 70})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\",  # {}안의 값을 새로운 값으로 대입 가능\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c6fe9",
   "metadata": {},
   "source": [
    "## 2) 메세지 기반 프롬프트 작성\n",
    "\n",
    "- BaseMessage 리스트\n",
    "- BaseMessage 상속 받은 클래스 : AIMessage, HummanMessage, SystemMessage, ToolMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9fb95d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Rome is not the capital of Italy, it's actually a city in Italy.\\n\\nThe capital of France is Paris.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T06:25:27.1230425Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2975091400, 'load_duration': 21470900, 'prompt_eval_count': 86, 'prompt_eval_duration': 1408187600, 'eval_count': 25, 'eval_duration': 1543931100, 'model_name': 'llama3.2:1b'}, id='run--a69b887e-efa1-4d1c-832e-019d542f1aa4-0', usage_metadata={'input_tokens': 86, 'output_tokens': 25, 'total_tokens': 111})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content = \"You ara a helpful assistant!\"),\n",
    "    HumanMessage(content = \"What is the capital of Italy?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content = \"What is the capital of Korea?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content = \"What is the capital of France?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc649362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Seoul.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# BaseMessage list로 하면 렝체인화 X, ChatPromptTemplate X\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content = \"You ara a helpful assistant!\"),\n",
    "    HumanMessage(content = \"What is the capital of Italy?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content = \"What is the capital of Korea?\"),\n",
    "    AIMessage(content = \"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content = \"What is the capital of {country}?\")\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(message_list)\n",
    "prompt = chatPromptTemplate.invoke({'country':'Korea'})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182eef4",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "\n",
    "- BaseMessage 리스트 → 튜플 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb175a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?중국\n",
      "프롬프트 : messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of 중국?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of China is Beijing.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You ara a helpful assistant!\"),\n",
    "    (\"human\", \"What is the capital of {country}?\"),\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":country})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecabbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?한국\n",
      "프롬프트 : messages=[SystemMessage(content='당신은 대한민국 전문 도우미야!', additional_kwargs={}, response_metadata={}), HumanMessage(content='한국의 수도가 어디예요?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요!\\n\\n한국의 수도는 Seoul입니다.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T06:51:36.9380607Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1132391400, 'load_duration': 22955000, 'prompt_eval_count': 43, 'prompt_eval_duration': 528407100, 'eval_count': 11, 'eval_duration': 579870800, 'model_name': 'llama3.2:1b'}, id='run--e5321653-79d4-4702-8f82-949d539eb4c5-0', usage_metadata={'input_tokens': 43, 'output_tokens': 11, 'total_tokens': 54})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 대한민국 전문 도우미야!\"),\n",
    "    (\"human\", \"{country}의 수도가 어디예요?\"),\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":country})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab7bd7",
   "metadata": {},
   "source": [
    "# 3. 답변 형식을 컨트롤하기\n",
    "\n",
    "- invoke 실행결과는 AIMessage() → String이나 json, 객체 : outputParser이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc926012",
   "metadata": {},
   "source": [
    "## 1) 문자열 출력 파서 이용\n",
    "\n",
    "- StrOutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d66999f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 :  text='What is the capital of Korea. Return the name of the city only'\n",
      "llm 결과 : <class 'langchain_core.messages.ai.AIMessage'> content='Seoul' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T06:52:36.4358242Z', 'done': True, 'done_reason': 'stop', 'total_duration': 604520500, 'load_duration': 27896900, 'prompt_eval_count': 39, 'prompt_eval_duration': 456836400, 'eval_count': 3, 'eval_duration': 118639800, 'model_name': 'llama3.2:1b'} id='run--3cf6551e-749f-441d-9235-7e7440f593e5-0' usage_metadata={'input_tokens': 39, 'output_tokens': 3, 'total_tokens': 42}\n",
      "파서 결과 : Seoul\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print('프롬프트 : ',prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print('llm 결과 :', type(result), result)\n",
    "\n",
    "# 문자열 출력 파서를 이용하여 llm응답을 단순 문자열 변환\n",
    "output_parser = StrOutputParser()\n",
    "print('파서 결과 :',output_parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8e16922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca3ff376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate(변수설정) → ChatPromptTemplate(변수설정, system과 모법답안 지정)\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant with expertise in South Korea.\"),\n",
    "    (\"human\", \"What is the capital of {country}? Return the name if the city only.\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1820ae9",
   "metadata": {},
   "source": [
    "## 2) 응답 타입 확인\n",
    "\n",
    "- json()으로 응답하기를 원하지만, 우선 어떤 형식으로 반환되는지 확인\n",
    "- {\"name\":\"흥\", \"age\":22}(json) / {\"name\":\"흥\", \"age\":22}(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cd73163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'> text='Give following information about Korea\\n    - Capital\\n    - Population\\n    - Language\\n    - Currency\\n    retrun it is JSON format and return the JSON dictionary only\\n    '\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'> content='```json\\n{\\n  \"capital\": \"Seoul\",\\n  \"population\": \"51,851,000\",\\n  \"language\": \"Korean\",\\n  \"currency\": \"Korean Won\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T07:02:44.7666107Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2768267000, 'load_duration': 32990000, 'prompt_eval_count': 62, 'prompt_eval_duration': 76098200, 'eval_count': 43, 'eval_duration': 2658347000, 'model_name': 'llama3.2:1b'} id='run--f7b617ff-6c44-4054-9ec0-20af1f47145d-0' usage_metadata={'input_tokens': 62, 'output_tokens': 43, 'total_tokens': 105}\n",
      "<class 'dict'> {'capital': 'Seoul', 'population': '51,851,000', 'language': 'Korean', 'currency': 'Korean Won'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "count_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    retrun it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt = count_detail_prompt.invoke({\"country\":\"Korea\"})\n",
    "print(type(prompt), prompt)\n",
    "\n",
    "# Json output 파서\n",
    "output_parser = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "print(type(prompt), ai_message)\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(type(json_result), json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcaffdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Seoul',\n",
       " 'population': 51.8,\n",
       " 'language': 'Korean',\n",
       " 'currency': {'code': 'KRW', 'symbol': '₩'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    retrun it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "info = output_parser.invoke(llm.invoke(count_detail_prompt.invoke({\"country\":\"Korea\"})))\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b477186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f395d",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "\n",
    "- Pydantic 모델을 사용하여 LLM 출력을 구조화된 형식으로 받기 (JsonParser보다 훨씬 안정적)\n",
    "- Pydantic : 데이터 유효성 검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a37fa8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x000001E92F0D4940>\n"
     ]
    }
   ],
   "source": [
    "class User :\n",
    "    def __init__(self, id, name, is_active = True) :\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "user = User(\"1\", \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c18dbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel) :\n",
    "    # gt = 0 : id > 0, ge = 0 : id >= 0, lt = 0 : id < 0, le = 0 : le <= 0\n",
    "    id:int = Field(gt=0, description = \"id\")\n",
    "    name:str = Field(min_length = 2, description = \"name\")\n",
    "    is_active:bool = Field(default = True,description = \"id활성화\")\n",
    "user = User(id = \"1\", name = \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "941157e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables = [\"country\"]    \n",
    ")\n",
    "\n",
    "class CountryDetail(BaseModel) :  # description : 더 정확한 출력 유도\n",
    "    capital:str = Field(description = \"the capital of the country\")\n",
    "    population:int = Field(description = \"the population of the country\")\n",
    "    language:str = Field(description = \"the language of the country\")\n",
    "    currency:str = Field(description = \"the currency of the country\")\n",
    "\n",
    "# 출력 형식 파서 + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "# output_parser = JsonOutputParser()\n",
    "# output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"})))\n",
    "\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cddb214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital='Seoul' population=51 language='Korean' currency='Won'\n",
      "Seoul 51\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print(info.capital, info.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dded41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json : {\"capital\":\"Seoul\",\"population\":51,\"language\":\"Korean\",\"currency\":\"Won\"}\n",
      "info를 dict : {'capital': 'Seoul', 'population': 51, 'language': 'Korean', 'currency': 'Won'}\n"
     ]
    }
   ],
   "source": [
    "print('info를 json :', info.model_dump_json())  # json()\n",
    "print('info를 dict :', info.model_dump())  # dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d394db",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 렝체인 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941d82a",
   "metadata": {},
   "source": [
    "## 1) 문자열 출력 파서 사용\n",
    "\n",
    "- invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09b7bcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:1b\", \n",
    "                 temperature = 0)  # 일관된 답변\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc8810",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "\n",
    "- 파이프연산자(|) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63958c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트템플릿 → llm → 출력파서를 연결하는 체인 생성\n",
    "\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfce93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
